---
title: The Incomplete Promise
author: Ali
layout: post
---

This week Stanford launched the [human-centered AI institute][HAI] (HAI), and to kick things off James Landay posted about the roles AI could play in society, and the importance of exploring smart interfaces.

I've followed HAI's development in passing, and I played the inaugural event in the background on Monday while I was doing other work. I study algorithmic systems that make important decisions about us - which I call "[street-level algorithms][sla]" in reference to Michael Lipsky's [street-level bureaucracies][slb] - and some of the work I've done in the past has taken a more careful look at historical parallels between things we see today (like [quantified self][qs] and [on-demand work][piecework]) to see if we can learn anything useful either for making sense of phenomena or for informing the design of systems.

All of this is to say that I took some interest in James's post a week ago, since it offered some references to a number of turning points in history to try and contextualize the launch of the HAI. That sort of stuff is kinda _my jam_.

What I got, though, was troubling. A series of anecdotes and speculative fictions that don't, in my opinion, adequately explore either the full gamut or even a satisfying range of aspects that we should be thinking about. I wanted to write some of the thoughts that came to mind to catalog misgivings I had when I read this post if for no other reason than to signal to others that, if you were nervous about that post, don't worry too much - so was I.

---

James opens with a story of an office that knows when your mood shifts, senses when you're stressed, and alters the ambiance accordingly to keep you in a state of arousal throughout the day. This, James promises, is "a glimpse of the true potential of AI". Fair enough. All I can think about as I read that story is how [blue light suppresses our bodies' production of melatonin][bluelight] (the chemical our body uses to regulate sleep); or how companies have begun rolling out policies requiring employees to submit to tracking of their personal activity, lest they incur higher health insurance premiums. What James describes necessarily involves some amount of tracking of things like your posture, eye movement, and other characteristics to operationalize this construct we call "fatigue". Do you feel comfortable being measured, evaluated, and ultimately managed by your employer at that level? [Teachers in West Virginia answered that they did not][wvstrike], pushing back on a policy roll out that seemed all but assured.

There's something subtle about the story here that I haven't mentioned - that "the thing" we're optimizing in our lives is *productivity*. This point is made surprisingly subversively, but it's worth rebuking more vocally: I don't care about how productive people are; I only care about how *happy* people are. I believe that the two ideas are deeply related - doing meaningful work certainly makes me happy - but these are not interchangeable ideas. More to the point, optimizing work for the sake of optimizing work is not a path toward happiness. Ask any of the people who worked under scientific management, or Fordism.





We talk a lot about the potential for AI to improve our lives, but we don't talk about the structures that exist today that either motivate, stymie, or otherwise mediate those promises.
This troubles me, because I know that we're not that naive, which leads me to wonder if we're deliberately not talking about these things to avoid some uncomfortable realities - that the way we're going today, we're talking about tracking, analyzing, and ultimately influencing our own behavior in service of... _something_.

James Landay, professor at Stanford and faculty associate director of the institute for Human-Centered Artificial Intelligence, wrote about some of these promises and the historical parallels that we see today. Since _that's sort of my jam_, I felt compelled to dive in. What resulted was more a running list of arguments that, troublingly, leave part of the history untold. I think we need to talk about these things if we're going to have a serious conversation about the crossroad we're at today, especially if we're going to evoke historical examples to make some conclusion.

## Human-centered AI initiatives that presuppose AI's existence are not "human-centered"; truly human-centered design presupposes nothing except the human.


[wvstrike]: https://www.thenation.com/article/the-west-virginia-teachers-strike-shows-that-winning-big-requires-creating-a-crisis/
[bluelight]: #
[slb]: #
[sla]: #
[qs]: #
[piecework]: #
[HAI]: #