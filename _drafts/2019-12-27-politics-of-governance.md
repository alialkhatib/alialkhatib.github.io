---
title: "The Human Factors of Human Data"
author: "Ali"
layout: "post"
---

So I've been thinking a lot about how to articulate a specific problem in researching, developing, and deploying AI.
I could give a book report on Dr. Ruha Benjamin's book *Race After Technology*, but if you follow me on Twitter you're probably sick of me talking about it.
Let's let it suffice to say that everything we build is, in a really deeply meaningful way, a product of the culture in which it was constructed.
And we should think deeply about the artifacts we construct, and what these artifacts say about our values and intentions and goals.


What I've been thinking about takes a left turn from this thread of work.
I've been thinking about the HCI community as a whole lately, and some conversations that never felt satisfyingly resolved.
One of the conversations that I won't talk about in this post is the ambiguity of what we consider "canonical HCI scholarship".
What do I expect *every* HCI researcher to have familiarity with?
I come from Anthropology, where I think it'd be fair to say that 



Coming up with an introduction is impossible whatever let's just...




I think something weird happened that became really salient at NeurIPS this year, and it'll be interesting to see how the HCI community weathers this.


A lot of what I do is look for ways to make sense of stuff going on today with the language of history.
Or to be more careful about it, I try to frame things happening today as events that unfolded after things that happened in the past.
When I say it like that, it seems kind of stupid that I have publications.
But hey, here we are.

It's probably some kind of payback on the universe's part - or just my own stubbornness - that I was a terrible student all the way through high school but a *particularly* bad student of history.
Anyway, that's neither here nor there.



Anyway, something that's been 

Something I do a lot of is situating things in historical backdrops to help make sense of stuff.



Over the past few months I've been thinking about stuff.
What's unique, or broken, or different, or... something, about the mess we're dealing with in AI?
I'd like to take a shot at articulating it.
The gist is that AI comes from an intellectual background that is ill-suited to its own work, and backfilling on that dearth is proving to be hard work; and secondly that a dangerous community within the core AI field doesn't even acknowledge this.

Jeff Bigham has said that

> The two hardest problems in computer science are: (i) people, (ii), convincing computer scientists that the hardest problem in computer science is people, and, (iii) off by one errors.

Generally speaking, I think this kind of points to the crux of the issue.