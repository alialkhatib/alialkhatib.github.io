Tech companies have sold the world a utopian vision of the future.
With AI, they promise [better medical care][], [more just treatment in courts][], [alleviated infrastructure][], [emotional support][], ways to achieve [environmental justice][]... The list goes on. Every problem in society seems to have been waiting for some smart agent to appear and find a line of best fit to satisfy everyone's needs. All we need to do is submit every piece of information that we can think to codify, train a model on that data, and let the machine make the decisions for us. In many cases, AI researchers warn, the decisions will be unconventional, even inscrutable. This is a problem that they're working on, they promise.

The application of AI in various aspects of society has shifted power around in ways that are hard to speak broadly about, except to say that often these technological shifts draw power away from the people the algorithms act upon, and consolidate power in the hands of those already in power. Those who define the data, those who write the code, those who frame the discussions themselves. To reclaim ownership over the problems we need to face, I think we need to reclaim those discussions.

I've borrowed a theory from political science, [street-level bureaucracy][], to describe the algorithmic systems that make decisions about our lives. I called them [street-level algorithms][], and through that theory I described how AIs can't understand the new and nuanced data that appears when they're deployed into the real world, because they can't understand *anything*, and the patterns traced out during training can never keep up with our growth and development as a society. That it will always be a step and a half behind us, always failing us.

These street-level algorithms construct a model of the world that's supposed to describe how the world works. Maybe [we train it on carceral data and hope that the system can describe who should be let out on bail and who shouldn't][bail], based on the training data we've provided. Maybe [we train it on fitness data and hope that it can predict health outcomes][health]. Maybe [we train it on historical education performance and project outward how students from that school might have performed if they had taken a test that had to be canceled][testing].

In all of the cases I pointed to just now, AI failed spectacularly and seemingly in unique ways, but I don't think we can continue to believe that. I think it's more accurate to say that AIs are trained on very narrow slices of data that were collected in social contexts that cannot be captured, let alone succinctly described, within the data itself. The things we leave out leave us oblivious to the myriad ways people live differently from our expectations, and the absence of the context of that data prevents AI now, and maybe ever, from recognizing that.

When we apply machine learning to court data, do we have a way to encode the racist decisions behind how we classify various kinds of drug crimes - to say nothing of what we decide to label a crime in the first place? Do we have a way to formalize into the system a history of police in the United States as enforcers of slavery and Jim Crow? Are we adequately accounting for the proclivity of judges to send Black men to prison to staff an institution of slavery that persists in America to this day?

Frank Wilderson III said once that "The brown person [for instance] embodies the demand of the return of the Southwestern United States; but when the Black person enters, here is the embodiment of the demand for which words can never express."







[better medical care]: #
[more just treatment in courts]: #
[various solutions to infrastructural congestion]: #
[better medical care]: #
[emotional support]: #
[environmental justice]: #