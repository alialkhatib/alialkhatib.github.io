This has been a distracting thought for a while and I wanted to put it into a document somewhere once I found the words that might be in the right neighborhood.
I think critics talk about the biases that people bring and either approach it like engineers (where bias disqualifies a thing) or they approach it like anthropologists (where bias contextualizes a thing).
I've been reading a lot of people complain that it was ignorant or naive of anyone to trust Google employees because they've always been biased, always been subject to Google's prerogatives, that these issues apply equally to industry in general;
I'm not exactly a champion of industry research, but
I think this kind of critique is too clumsy, and a more careful outline of whether and how biases contextualize a person's work or disqualifies their work would be helpful until we can draw a starker boundary for independent scholarly work.
If nothing else, I want to stop hearing cynics tweet that they were so much smarter than the rest of us in that they *never* trusted industry researchers, because that's demonstrably not true.

Let's start with the hot take about cynics. If you've been in my mentions about how you never trusted industry researchers, even in response to my latest questions asking how I can trust Google employees not to put their thumb on the scale while they're anonymously reviewing critical submissions, then I would like to see screenshots of how far back you started indicating to area chairs of conferences that your submission was conflicted with *all industry-affiliated researchers* and that it couldn't be reviewed by anyone affiliated with industry at the time.
My hunch is that that's vanishingly few, if any, of you. I've never seen it, never heard of it, and I suspect such a caveat would have broken the review process for a lot of conferences.
I say this so that I can tell you that I think that we were pretty aligned from the start in that people's biases were *contextualizing* but not necessarily always *disqualifying*.
I think if we were on the same page before, you might be with me in my reasoning as I unpack the difference and how Google has measurably shifted these boundaries, and why it's unhelpful to effect such a cynical or numbed attitude to this.

I've been trying to unpack this thought for a while now; longer than this month, longer than this year. To what extent can I trust that I would have intellectual freedom at any given institution - whether that be in academia or industry - to challenge what the institution is doing or just to express skepticism about the company's agenda?
I know that non-research employees have many more constraints on what they can talk about, what views they can express, and how/where/to whom they're supposed to be allowed to express those views.
I don't have much more insight than that. On the research side, however, I've always inferred that people who wrote papers for conferences had *some* degree of latitude. Of course I assume that PR would tell employees to speak positively about the company; the question is whether, how, and where that kind of directive is enforced.
I admit that I naively assumed that, in practice, researchers at a company like Google were mostly free to express skepticism about the general trends in which Google participates - for instance, training monolithic machine learning models that consume vast amounts of energy and perform pretty poorly at the margins.
I think Dr. Gebru assumed the same or she and her colleagues wouldn't have submitted their paper to FAccT.
Journalists have detailed how other employees at Google and elsewhere found the policies surrounding paper submission were applied so strictly to the Stochastic Parrots paper that they could think of no other cases where the book had been thrown at a researcher in this way.

<!-- Importantly, I think that couldn't be expressed by company policies. I don't know how structurally it would work for an institution to outline the ways that some people are allowed to critique some aspects of the company; the only conclusion to that line of thinking is to say that everyone should be allowed to question, critique, and ultimately discuss the direction and "mission" of the company. Almost as if everyone should be involved in the governance. Almost. -->

<!-- I'll digress from collective governance. -->
The point is that these rules have always existed, and I don't really think anyone is surprised to discover that rules and talking points exist.
What's critical is that those rules have always been variably enforced, and my understanding was that researchers tacitly assumed they were allowed to use their discretion to submit and ultimately publish work that had merit, even if it was more than a little tangentially critical of the company.
I think Dr. Gebru also operated on that assumption; otherwise she and her colleagues wouldn't have even bothered to submit the "Stochastic Parrots" paper to FAccT (let alone worked at Google in the first place). I think the shock that Google employees are expressing - pointing out that they've never endured the kind of paper review that Dr. Gebru and Jeff Dean both describe in their accounts of this paper's review process.
The rules have hardly, if ever, been applied this thoroughly and stringently, until now with a Black woman doing critical scholarship.

(I don't think every company has the same set of boundaries; I certainly think some companies have stricter rules on candor than others (hi, Apple, it's been nice seeing you very occasionally at CHI lately), and some places probably have boundaries that are bizarrely specific (hi, Uber). But I suspect the legal stuff people sign when they join any of these companies all looks pretty similar, and the difference is in actual on-the-ground enforcement, informed by things like the person's status and identity. That status quo had always been unjust, but it was predictable, in a way that I now realize reminds me of how my family talked about Saddam's regime compared to the chaos that followed the Iraq War)

I think the companies knew that enforcement was where all of this mattered, because outlining the parameters within which an employee (even a research employee) could express dissent and challenge the PR language would have been all but impossible. I assumed that companies largely declined to enforce policies when it came to researchers, and that this practiced neglect was part of what allowed researchers to do their work without interference from corporate. And I suspect that was the way most of us, even in academia, assumed things worked. Could I ask a Google researcher to come give a talk about Safiya Noble's book *Algorithms of Oppression*? Honestly, probably not. But if Alex Hanna or Margaret Mitchell gave a 15-minute talk at CHI during the regular paper sessions, and I asked them something that necessitated a comment on *Algorithms of Oppression*, I would have expected that they would endorse Noble's work, as everyone I know who does credible critical work would. In other words, I think the rules wouldn't have been enforced as strictly in certain crucial contexts, and I think that their behavior up until now (being informed about critical work, & respecting and promoting good critical work - even work that criticizes Google) has reflected that belief as well.

In this way, I think that the biases they brought to the table were *contextualizing*, but not necessarily *disqualifying*. I think it speaks volumes about someone for them to think that Facebook is the best focal point for the kind of research that they want to do - maybe as a social network analyst they feel that the most important questions are best answered at Facebook rather than at some other company leveraging some other company's data, or in academia where access to any data would be more complicated. I think it says a lot about someone to work at a tech company rather than at a policy or advocacy group; it says that the place where they can do the most good is building tools, rather than advocating for legislative solutions. These biases all contextualize how someone frames problems and what language they speak, or what tools they use, when they try to talk about solving those problems.

Sometimes a person's contextualizing bias becomes disqualifying. Someone paid by Uber can't credibly do research that centers drivers. The scholarship they produce may talk about the economic impact that ridesharing has on a city, and in that sense their status as an employee or affiliate of Uber substantially shades their work, but the fact that we critique and deconstruct that work is evidence that it's different from a paper where they envision how to protect drivers' rights.
Dr. Gebru's paper advocating against the use of monolithic algorithmic systems would have been a surprising recommendation from someone at Google, and I would have situated that work in the context of her employer and the circumstances surrounding her work, but it also would have influenced my perception of the extent to which Google is willing to let researchers draw conclusions that hurt them in the short term, recognizing that knowledge in general benefits them in the long term.

Microsoft Research does this competently; Mary Gray wrote the book on *Ghost Work*, the mode of work that goes into training machine learning systems that inform all of the AI applications that Microsoft (the corporate entity) engages in. They're willing to support Dr. Gray's work because they recognize that even if the medicine is bitter right now, they need to take it; they need to hear it because in the long term it keeps them tethered to reality. There are plenty of critiques to make about Microsoft, and of Microsoft Research, but in general the kind of work that comes out of MSR is biased in ways that contextualize rather than disqualify. Not always, but mostly.

And please, academics, shut the fuck up if you think you're immune to this. If you think power and money and prestige don't leverage the kind of work that gets valued and devalued in universities, talk to Black women who get denied tenure for doing critical work, or the Black studies, gender studies, and other social sciences and humanities programs that have been shuttered to keep STEM alive. Or talk to the title IX offices that find ways to conclude that there's too much ambiguity surrounding the many sexual harassment claims against faculty members who bring millions of dollars to the university. Don't say that the institution of academia doesn't put its thumb on the scale. It's just a different thumb.

You still need to do the work of situating how a CS researcher would frame and approach a problem differently than a law & policy researcher would. You still need to recognize how seemingly straightforward collaborations between departments and groups bely power dynamics playing out behind the scenes within and between those groups. If you think you can cite academia without any effort, you're a fool.



<!-- press releases, which we mostly ignore unless they make pretty specific claims that we need to rebuke.
The context that this researcher was sponsored by Uber may even lead us to omit their work from our own, or it may lead us to cite their work, adding caveats and context in the paper we're writing to communicate how skeptically a reader should take the findings of the thing we're talking about. -->

<!-- I assume that everyday tech workers are expected to generally be positive about where they work (or to be silent about it), which is part of the motivation for tech worker advocacy and organizing - people who work at Facebook should really be able to voice their dissatisfaction and pessimism about Facebook's effects on society. Without that candor, it'll be much more difficult to peer into how Facebook arrives at policies and decisions that affect the billions of us captured by Facebook's products. And that's why it's so important that employees have spoken anonymously with journalists - because right now that's the only view we really get into how screwed up things are. Mark Zuckerberg, Adam Mosseri, and others don't govern transparently, don't understand the value or importance of things like democratic processes, and so create the need for employees to speak anonymously. -->