---
title: AI Kremlinology
---

I've been trying to dig myself out of a pretty deep depression for some time now, and while I still don't feel like I'm out of it, I have at least found some clarity on where some of this depression is coming from. I mean... in addition to all the other stuff.

A while back I wrote this paper where I was trying to describe the kinds of violence that I was observing in complex algorithmic systems - let's call them “AI” or “machine learning systems”, but this will all probably apply more broadly than either of those narrow marketing terms. It seemed like people were experiencing a lot of violence that wasn't mapping easily to the kind of naive sense of harms that were popular to talk about at the time, so I was looking for something that could kind of… bring into focus an oblique angle of what felt like something cutting deeply, but not along the dimensions that we were used to talking about stuff.

I was ultimately writing about bureaucratic theory and administrative systems. The idea of an administrative state - and all the rhetoric, theory, and insights about administrative violence - being brought to bear on seemingly new algorithmic systems really appealed to me, because it made sense of a lot of the kinds of harms that otherwise seemed hard to parse from the perspective of an ahistorical computer scientist.

It also appealed to a sense I had, that these systems were probably new in the strict sense… but didn't feel new in the sense that we were seeing black people denied bail, or disabled people rejected from jobs, or trans people run out of spaces for talking about gender identity… all stuff that was decidedly pretty un-novel.

Historically minoritized, and particularly vulnerable people, arguably understand how bureaucratic systems work, certainly better than any political scientist without direct experience does. To reference a recent episode from the 5-4 podcast, it's not totally surprising that jay-z predicted the incomprehensible rationale of drug-sniffing dogs being ruled as not infringing on your constitutional protections against unreasonable violations of dignity by way of roadside searches; he's a black man, and in that way he understands the reality of the law better than any of the half-dozen legal scholars who got quoted in the New York Times a while ago who were all having various existential crises over the Supreme Court's more transparent politicization. Constitutional law scholars don't have an experience with the law as quotidian, as visceral, as decisive for their lives as Jay-Z does.

But I digress; the point is that people with real experience pressed up against administrative violence understand administrative violence in a lot of the same ways that the same people understand how AI systems project and create harm.

At the end of my paper, I pointed out that two things make this algorithmic harm really possible:

- the capture of vulnerable people within those systems, often against their will; and
- the lack of consequences for the system making the "wrong call"

I think I didn't unpack my condemnation for systems that both capture people in their domains and also have no consequences for doing terrible things to those people, but I felt confident that I was painting a sufficiently clear picture of a kind of incoherent, senseless, bureaucratic dystopia - a utopia *for the systems themselves* rather than for us - and I hoped that people would pull back from that cliff upon seeing the precipitous fall.

Instead, it sort of seems like people embraced that incoherent, senseless, bureaucratic dystopia… and dove right in.

In the time since then, the ascent of generative AI has turned a lot of human processes and activities like learning; art; basic correspondence; as well as job, rental, and school applications; and myriad other things into a mess of noise and garbage. And it doesn't seem like that's getting any better, to be honest.

For some of that time, I was director of the Center for Applied Data Ethics, where I had some space to think about and try to engage with this stuff. But wave after wave of AI systems, whether for generative or discriminatory/classification purposes, made everything seem increasingly absurd and senseless. These systems weren't constructing worlds that made sense for underlying reasons; they were creating rules without reasons or rationales behind any of them, purely informed by the observations from datasets not designed for these applications.

Maybe some part of that is the depression - this feeling I've had that everything is moving so fast, and is so relentlessly shitty, that it defies and escapes descriptions and words.

This might be obvious to you, or it might be incomprehensible: I felt like my job was to think clearly about things, and then to write stuff. And I didn't feel like I could think clearly, partly because it seemed like everything was terrible, and getting worse at an accelerating rate. And I think it's safe to say… that makes the depression worse.

None of this is the insight. Welcome to where my head was at like… 2 years ago.

Over time, I felt haunted by the looming sense that these AIs were like a kafkaesque nightmare. There was nothing that made sense about why these systems made any decisions that they did - it was just arbitrary, stochastic garbage. They just sort of optimized for a pattern that they observed. And again, rather than pull back from the meaningless, senseless, dystopian bureaucracy that I felt was pretty clearly awful… A lot of people, and a lot of tech companies, and a lot of governments all seemed to be lured in by the tantalizing prospect of a bullshit generator of their own, finally doing the work for them.

---

In 2009 or 2010, I remember it seemingly wasn't as common to file tax returns online as it is now. Maybe it's more accurate to say it was something I wasn't aware I could do. I remember filling out my tax return information, printing out my documents, and *driving* in my car over to the nearest regional processing facility - this huge industrial building with countless shipping trucks coming and going - where you could drop off the mail a few hours later than all the more local post offices and mailboxes. Your tax returns wouldn't be counted as late if they were just postmarked on the day, so as long as you dropped it off in time, you would be okay.

The San Jose Processing Facility was a bit far, maybe 30 or 40 minutes away if there wasn't traffic, or about an hour if there was. But I can't put into sufficient words the kind of chaos that you would have seen if you visited the San Jose Processing Facility at 11:30pm on April 15th on any given year. They used to be open late on Tax Day, because ***that was the world we lived in***.

People stopped their cars in the middle of the street, and ran up to the wrong side of the building where all the trucks were, with their tax returns held above their heads, as if desperately looking for a waiter or something. This wasn't a building that regular people were necessarily supposed to access all the time, after all. When I say “USPS regional processing facility”, whatever industrial-looking picture you're thinking of in your head is probably not industrial enough. So anyway, finding the correct entry point to drop off mail in the middle of the night was… a difficult process. And there were tons of cars driving around, stopping in the middle of a busy multi-lane street, more focused on getting their tax returns through the door than any other earthly thing.

---

I think my depression came from the intuition that documenting and describing the peculiarity of AIs that cause harm seems a little too… peculiar to me. I know and respect a lot of the people who are leading the way on AI auditing work, but writing papers about auditing algorithmic systems is a prospect that I could best describe as "Kremlinology of AI". The idiosyncratic, outsider observations of what little clues the AI systems dropped, and a lot of the writing about how and why these systems did what they did… felt a little too speculative and uncertain; and worse, it felt like anything I wrote about how these systems worked (or didn't work) could be modified and rendered irrelevant in the span of a few hours.

I don't really want to write about the strange and exceptional traffic patterns outside the post office distribution center in San Jose on April 15th. I'm not saying it wasn't chaotic, or dangerous; I'm certainly not saying that it was unimportant. But even as I write this out, it feels like I'm a strange old man telling you about how people used to have ice delivered every week for their fridges. Like, okay, that's wild, but I don't know what to do with that story anymore because fridges haven't worked that way for like a hundred years. It's such a dated and strange anecdote, and it lost relevance rapidly as electric refrigerators and other circumstances replaced them.

I could write about how ChatGPT or Claude or Gemini or whatever else can't give you an accurate count of the countries in Africa that have a K in their names. But then, within… an hour? Maybe less? someone at one of these companies could push an update that renders that entire analysis meaningless. And it would be worse than meaningless, because describing how the system works or doesn't work is *the currency* that I'm offering. If I don't have the facts, or if I'm describing a system that doesn't exist and never existed long enough for people to even remember it, then it's perhaps even worse than the story about the post office shenanigans or the refrigerator and the ice blocks. At least some people remember that shit.

---

All this is to say:

I feel like AIs are themselves a a kind of senseless, kafkaesque nightmare where rules exist for no intrinsic reason except perhaps to justify the system's use and the power we give to that system; and it is that power itself that makes the rules have any bearing on reality, not the adherence of those rules to the lived reality of anyone.

And also, AIs, or more accurately the people and companies putting AI into everything… are making the world an increasingly senseless, nightmarish place. They're not the only thing making the world senseless and nightmarish, but they're pulling their weight.

And finally: the idea of settling into a career where I document and speculate about little particular details of these senseless, arbitrary, unhinged-from-reality systems… just feels hopelessly depressing. Like… I don't want a career in “AI Kremlinology”.

I have no idea where or how this piece was supposed to end. I actually do have some thoughts that I'd love to share, depending on how pitches and proposals go. One of them is… I wouldn't say optimistic, because look at me. But I would say that it has a target on the horizon that I advocate working towards, rather than a spirit looming over me, haunting and hectoring me to keep running away.